{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3067ff33-be7a-4d2d-8f48-47aadf250de9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This notebook was for acquring the track details for all the pickels in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b12fd9-14fd-4dce-af48-0ff37ea01eab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setting up general imports and boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b650e09-53c5-4adb-964b-688d0d16a0e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# !pip install boto3\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# AWS Credentials and Settings\n",
    "access_key = 'ACCESS_KEY'\n",
    "secret_key = 'SECRET_ACCESS_KEY'\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
    "encoded_secret_key = secret_key.replace(\"/\", \"%2F\").replace(\"+\", \"%2B\")\n",
    "\n",
    "aws_region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name=aws_region,\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b04335f2-fa41-4889-a312-af8e36fb46fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Setting up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8773d2-903f-4392-a9c6-e4004362cf61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark Setup\n",
    "# Setting up Spark\n",
    "# !pip install pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession, Row, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, explode, col, collect_list, regexp_replace, split, expr, length, concat_ws, count, size, first\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, FloatType, DoubleType\n",
    "from pyspark import sql\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# Set up Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PicklesPlus\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\",\"s3.\" + aws_region + \".amazonaws.com\") \\\n",
    "    .config(\"spark.executor.memory\", \"15g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.default.parallelism\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "958a8723-cd52-442a-a20d-01f5da165479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Importing ML stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a1cb6c-77f7-4076-b440-7abca04e9a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing ML libraries\n",
    "from pyspark.ml.stat import Correlation, ChiSquareTest, Summarizer\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, Tokenizer, HashingTF, IDF, VectorAssembler, StandardScaler\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb4bb51-b630-4189-8a2f-f53e02237d9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005ac5e7-b153-4596-9e52-289a696b0832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main = spark.read.parquet('s3a://kagglespotify6k/raw/main_dataset.parquet/')\n",
    "\n",
    "tracks_parq = spark.read.parquet('s3a://kagglespotify6k/raw/pickles/pickle_tracks_test.parquet/')\n",
    "\n",
    "# main contains some duplicate columns\n",
    "columns_to_drop = ['key', 'loudness', 'mode', 'tempo', 'time_signature']\n",
    "main_merge = main.drop(*columns_to_drop)\n",
    "\n",
    "# Join the 2 dataframes\n",
    "merged_df = tracks_parq.join(main_merge, tracks_parq['track_name'] == main_merge['track_uri'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f18cef7d-a42e-4854-9353-1ac1160d4bd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Getting all the pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814125a4-c186-4a99-932f-7bc82a994fea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277923\n30544\n247379\n247379\n"
     ]
    }
   ],
   "source": [
    "all_tracks = main.select(col('track_uri')).collect()\n",
    "track_list = [row['track_uri'] for row in all_tracks]\n",
    "present_tracks = merged_df.select(col('track_uri')).collect()\n",
    "present_list = [row2['track_uri'] for row2 in present_tracks]\n",
    "\n",
    "missing_tracks = [track for track in track_list if track not in present_list]\n",
    "\n",
    "print(len(track_list))\n",
    "print(len(present_list))\n",
    "print(len(missing_tracks))\n",
    "print(len(track_list) - len(present_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d884f4-a36a-4761-abe0-d41a55fb1d49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# test_set = missing_tracks[0:5000]\n",
    "\n",
    "track_attr = ['duration','loudness','tempo','tempo_confidence','time_signature','time_signature_confidence','key','key_confidence','mode','mode_confidence']\n",
    "\n",
    "# Attempt 1\n",
    "def get_track(pk_file):\n",
    "    s3 = boto3.client('s3')\n",
    "    curr_pickle = s3.get_object(Bucket='kagglespotify6k', Key=f'landing/Cleaned Analyses/Cleaned Analyses/{pk_file}.pickle')\n",
    "    pk_read = pickle.loads(curr_pickle['Body'].read())\n",
    "    track_data = pk_read['track']\n",
    "    filtered_track = {key: track_data[key] for key in track_attr if key in track_data}\n",
    "    filtered_track['track_uri'] = pk_file\n",
    "    return(filtered_track)\n",
    "\n",
    "# test_rdd = spark.sparkContext.parallelize(test_set)\n",
    "# track_gets = test_rdd.map(get_track)\n",
    "# fin_track = track_gets.collect()\n",
    "# rows = [Row(**data) for data in fin_track]\n",
    "# df = spark.createDataFrame(rows)\n",
    "# df.show()\n",
    "\n",
    "# Took 4 seconds for 10 records\n",
    "# 14.78 seconds for 100 records - 6.76 per second\n",
    "# 1.61 minutes for 1000 records - 10.35 per second\n",
    "# 25.41 minutes for 10,000 records - 6.55 per second\n",
    "# 7.74 minutes for 5000 records - 10.76 per second\n",
    "\n",
    "# Instead of making a data frame each time or shoving it into a dataframe for each iteration\n",
    "# Store it all in a dictionary or list, then create a dataframe at the en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6859955-9a2e-45cd-9ae4-b531113329e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing_section_1 = [missing_tracks[0:5000], missing_tracks[5000:10000], missing_tracks[10000:15000], missing_tracks[15000:20000], missing_tracks[20000:25000], missing_tracks[25000:30000]]\n",
    "\n",
    "missing_section_2 = [missing_tracks[30000:35000], missing_tracks[35000:40000], missing_tracks[40000:45000], missing_tracks[45000:50000], missing_tracks[50000:55000], missing_tracks[55000:60000]]\n",
    "\n",
    "missing_section_3 = [missing_tracks[60000:65000], missing_tracks[65000:70000], missing_tracks[70000:75000], missing_tracks[75000:80000], missing_tracks[80000:85000], missing_tracks[85000:90000]]\n",
    "\n",
    "missing_section_4 = [missing_tracks[90000:95000], missing_tracks[95000:100000], missing_tracks[100000:105000], missing_tracks[105000:110000], missing_tracks[110000:115000], missing_tracks[115000:120000]]\n",
    "\n",
    "missing_section_5 = [missing_tracks[120000:125000], missing_tracks[125000:130000], missing_tracks[130000:135000], missing_tracks[135000:140000], missing_tracks[140000:145000], missing_tracks[145000:150000]]\n",
    "\n",
    "missing_section_6 = [missing_tracks[150000:155000], missing_tracks[155000:160000], missing_tracks[160000:165000], missing_tracks[165000:170000], missing_tracks[170000:175000], missing_tracks[175000:180000]]\n",
    "\n",
    "missing_section_7 = [missing_tracks[180000:185000], missing_tracks[185000:190000], missing_tracks[190000:195000], missing_tracks[195000:200000], missing_tracks[200000:205000], missing_tracks[205000:210000]]\n",
    "\n",
    "missing_section_8 = [missing_tracks[210000:215000], missing_tracks[220000:225000], missing_tracks[230000:235000], missing_tracks[235000:240000], missing_tracks[240000:245000], missing_tracks[245000:247379]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c28b78-bbe1-41d6-9a5b-61b10d060ccf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fin_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a634ac6c-0e24-4ebf-8a4e-17ebce82de78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for section in missing_section_1:\n",
    "    list_rdd = spark.sparkContext.parallelize(section)\n",
    "    track_gets = list_rdd.map(get_track)\n",
    "    fin_track.extend(track_gets.collect())\n",
    "\n",
    "# 47.31 minutes - ~30,000 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbdc2e5-8241-4577-8b6c-c0ccd30b3292",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for section in missing_section_2:\n",
    "    list_rdd = spark.sparkContext.parallelize(section)\n",
    "    track_gets = list_rdd.map(get_track)\n",
    "    fin_track.extend(track_gets.collect())\n",
    "\n",
    "# 46.60 minutes - ~30,000 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9316a5-fd77-4f96-8806-fd10ccfbbc5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for section in missing_section_3:\n",
    "    list_rdd = spark.sparkContext.parallelize(section)\n",
    "    track_gets = list_rdd.map(get_track)\n",
    "    fin_track.extend(track_gets.collect())\n",
    "\n",
    "# 47.61 minutes - ~30,000 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31811739-bdb2-4bcb-a881-9af4ddabaa6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+----------------+--------------+-------------------------+---+--------------+----+---------------+--------------------+\n| duration|loudness|  tempo|tempo_confidence|time_signature|time_signature_confidence|key|key_confidence|mode|mode_confidence|           track_uri|\n+---------+--------+-------+----------------+--------------+-------------------------+---+--------------+----+---------------+--------------------+\n|217.57333|  -8.029|170.044|            0.05|             4|                      1.0|  0|         0.601|   0|          0.574|2CY92qejUrhyPUASa...|\n|    253.0|  -4.999| 75.003|           0.195|             4|                    0.627|  0|         0.494|   1|          0.473|73xsMXuRNB3yqLeNc...|\n| 216.1868|  -4.842| 83.571|           0.564|             4|                     0.99|  7|         0.061|   1|          0.405|6TwrBbgTaB5gpl06Y...|\n| 123.6298|  -4.233|182.912|           0.143|             4|                      1.0|  7|         0.712|   0|          0.691|37tp9qtF4YQF0nvaQ...|\n|    184.4|  -5.593| 96.039|           0.527|             4|                    0.925| 10|         0.594|   1|          0.576|2RKmGDTq840oLpKC0...|\n+---------+--------+-------+----------------+--------------+-------------------------+---+--------------+----+---------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "rows = [Row(**data) for data in fin_track]\n",
    "missing_tracks_df = spark.createDataFrame(rows)\n",
    "missing_tracks_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6edc031-289d-44ac-a2fb-8d3b94864cd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing_tracks_df.write.mode('overwrite').parquet('s3a://kagglespotify6k/trusted/tracks_data_0_to_90k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d05fc4-47c1-4e50-a7a1-5bc19e2581a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for section in missing_section_4:\n",
    "    list_rdd = spark.sparkContext.parallelize(section)\n",
    "    track_gets = list_rdd.map(get_track)\n",
    "    fin_track.extend(track_gets.collect())\n",
    "\n",
    "# 49.03 minutes - 30k records\n",
    "# 47.24 minutes - 30k records\n",
    "# 46.18 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42618a31-4d20-4be1-b070-e5b20aead678",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for section in missing_section_5:\n",
    "    list_rdd = spark.sparkContext.parallelize(section)\n",
    "    track_gets = list_rdd.map(get_track)\n",
    "    fin_track.extend(track_gets.collect())\n",
    "\n",
    "# 56.86 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c5580c-2811-458d-9d30-9c568dcc55bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " for section in missing_section_6:\n",
    "    list_rdd = spark.sparkContext.parallelize(section)\n",
    "    track_gets = list_rdd.map(get_track)\n",
    "    fin_track.extend(track_gets.collect())\n",
    "\n",
    "# 51.50 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58e2da3-27ec-4bff-b978-e18764d9aa5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for section in missing_section_7:\n",
    "    list_rdd = spark.sparkContext.parallelize(section)\n",
    "    track_gets = list_rdd.map(get_track)\n",
    "    fin_track.extend(track_gets.collect())\n",
    "\n",
    "# 47.49 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f639e7-4711-4d97-8466-a9b91aca17d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for section in missing_section_8:\n",
    "    list_rdd = spark.sparkContext.parallelize(section)\n",
    "    track_gets = list_rdd.map(get_track)\n",
    "    fin_track.extend(track_gets.collect())\n",
    "\n",
    "# 46.10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49424e7a-c31c-41e3-91fc-c75c2813472f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: 147379"
     ]
    }
   ],
   "source": [
    "len(fin_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7044b00-2dc9-4b1e-9002-673b349b220a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+----------------+--------------+-------------------------+---+--------------+----+---------------+--------------------+\n| duration|loudness|  tempo|tempo_confidence|time_signature|time_signature_confidence|key|key_confidence|mode|mode_confidence|           track_uri|\n+---------+--------+-------+----------------+--------------+-------------------------+---+--------------+----+---------------+--------------------+\n|425.33334|  -6.448|123.557|            0.29|             4|                    0.974|  3|         0.458|   1|          0.638|28Ri5diNoXNl0s3Ew...|\n|379.35126|  -4.573|171.812|           0.134|             4|                    0.732|  9|         0.752|   1|          0.503|6P5inyUmrJuslHrIN...|\n|213.70667|  -6.182|136.034|           0.857|             4|                    0.989|  6|         0.631|   1|           0.43|3yAw3cFOPzUPkklui...|\n|199.83887|  -4.064|115.825|           0.163|             4|                      1.0|  9|         0.682|   1|          0.475|3tYlMvSXo7APLad8u...|\n|   225.72| -11.398| 96.268|            0.59|             4|                      1.0|  0|         0.763|   1|          0.789|6Ysnd9T4uIhQesbWK...|\n+---------+--------+-------+----------------+--------------+-------------------------+---+--------------+----+---------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "rows2 = [Row(**data) for data in fin_track]\n",
    "missing_tracks_df2 = spark.createDataFrame(rows2)\n",
    "missing_tracks_df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6ae364-3143-4e62-a175-787250f3e49b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing_tracks_df2.write.mode('overwrite').parquet('s3a://kagglespotify6k/trusted/tracks_data_90k_to_270k')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pickle Collector",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
