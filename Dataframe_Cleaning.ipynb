{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "44927f59-8f8c-4e85-8048-f0f592c68174",
          "showTitle": false,
          "title": ""
        },
        "id": "x44B6CJuE1Gm"
      },
      "source": [
        "# General Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1ed99939-a7e2-4163-a2d5-3524ec1cb52d",
          "showTitle": false,
          "title": ""
        },
        "id": "fwcINwNYE1Gp",
        "outputId": "25157197-f4b6-4cae-b408-ea44d108242c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
            "text/plain": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Setting up the environment\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import os\n",
        "import time\n",
        "\n",
        "# AWS Credentials and Settings\n",
        "access_key = 'ACCESS_KEY'\n",
        "secret_key = 'SECRET_ACCESS_KEY'\n",
        "\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
        "encoded_secret_key = secret_key.replace(\"/\", \"%2F\").replace(\"+\", \"%2B\")\n",
        "\n",
        "aws_region = 'us-east-1'\n",
        "\n",
        "s3 = boto3.client(\n",
        "    service_name='s3',\n",
        "    region_name=aws_region,\n",
        "    aws_access_key_id=access_key,\n",
        "    aws_secret_access_key=secret_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cf4deefb-f755-429a-a9f5-29299046263e",
          "showTitle": false,
          "title": ""
        },
        "id": "cAthPwcBE1Gq"
      },
      "source": [
        "#Spark Stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e7c3e707-9363-4662-bb68-dbcef56ed362",
          "showTitle": false,
          "title": ""
        },
        "id": "uJ9PwcccE1Gr"
      },
      "outputs": [],
      "source": [
        "# Setting up Spark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import udf, explode, col, collect_list, regexp_replace, split, expr, length, concat_ws\n",
        "from pyspark.sql.types import ArrayType, StringType, IntegerType, FloatType, DoubleType\n",
        "from pyspark import sql\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "# Set up Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PicklesPlus\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\",\"s3.\" + aws_region + \".amazonaws.com\") \\\n",
        "    .config(\"spark.executor.memory\", \"15g\") \\\n",
        "    .config(\"spark.executor.cores\", \"2\") \\\n",
        "    .config(\"spark.default.parallelism\", \"4\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b69d0422-bef3-4155-a693-bee7d0f43df6",
          "showTitle": false,
          "title": ""
        },
        "id": "EFK5AiNlE1Gr"
      },
      "source": [
        "#CSV Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6a880875-6416-43b1-a21c-dfaaa2c24907",
          "showTitle": false,
          "title": ""
        },
        "id": "g5nWReI4E1Gs",
        "outputId": "e3ffd4ce-0aed-481e-af34-02ca34961e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artists.parquet\nfinal_playlists.parquet\nfinal_tracks.parquet\nmain_dataset.parquet\n"
          ]
        }
      ],
      "source": [
        "# Referenced https://stackoverflow.com/questions/70899029/how-to-get-all-rows-with-null-value-in-any-column-in-pyspark\n",
        "\n",
        "def clean_spark_df(df):\n",
        "    # Filters for null values\n",
        "    null_filter = F.exists(F.array(*df.columns), lambda x: x.isNull())\n",
        "    df_null = df.filter(null_filter)\n",
        "    # Removes those entries\n",
        "    df_fin = df.subtract(df_null)\n",
        "    return(df_fin)\n",
        "\n",
        "csv_paths = ['s3a://kagglespotify6k/landing/artists.csv', 's3a://kagglespotify6k/landing/final_playlists.csv', 's3a://kagglespotify6k/landing/final_tracks.csv','s3a://kagglespotify6k/landing/main_dataset.csv']\n",
        "\n",
        "for path in csv_paths:\n",
        "    file_load = spark.read.csv(path, header = True, inferSchema = True)\n",
        "    df_clean = clean_spark_df(file_load)\n",
        "    file_name = path.split('/')[-1][:-4] + '.parquet'\n",
        "    df_clean.write.mode('overwrite').parquet(f's3a://kagglespotify6k/raw/{file_name}')\n",
        "    print(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0031aefb-048d-4bfc-a368-642d2503da8a",
          "showTitle": false,
          "title": ""
        },
        "id": "crQj2Zc1E1Gs"
      },
      "outputs": [],
      "source": [
        "# The genre column in main_dataset is a string that looks like a nested list, will convert it to a normal list of strings\n",
        "main_df = spark.read.parquet('s3a://kagglespotify6k/raw/main_dataset.parquet/')\n",
        "\n",
        "# Remove brackets\n",
        "main_df = main_df.withColumn('artists_genres', regexp_replace('artists_genres', r\"[\\[\\]]\", \"\"))\n",
        "\n",
        "# Remove single quotes then split into individual genres\n",
        "main_df = main_df.withColumn('artists_genres', split(regexp_replace('artists_genres', \"'\", \"\"), \",\\s*\"))\n",
        "\n",
        "# The track_uri column contains an unnecessary sub-string in the front \"spotify:track:\", will remove it\n",
        "main_df = main_df.withColumn('track_uri', regexp_replace('track_uri', \"spotify:track:\", \"\"))\n",
        "\n",
        "# Overwrite to save\n",
        "main_df.write.mode('overwrite').parquet('s3a://kagglespotify6k/raw/main_dataset.parquet/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c7ee29ac-e27b-445f-a9a2-018cd4930f6c",
          "showTitle": false,
          "title": ""
        },
        "id": "EBoZXXY9E1Gs"
      },
      "source": [
        "#txt Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e1b62254-5986-48d8-89fb-db2400df35f3",
          "showTitle": false,
          "title": ""
        },
        "id": "wEoVwYMWE1Gt",
        "outputId": "13787228-1d54-406a-8879-6d70821b132f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im_getting_these_vibes_uknow.parquet\nmusic_genres.parquet\n"
          ]
        }
      ],
      "source": [
        "# Earlier EDA showed that the txt files do not have null values, but they do have some duplicates\n",
        "txt_paths = ['s3a://kagglespotify6k/landing/im_getting_these_vibes_uknow.txt', 's3a://kagglespotify6k/landing/music_genres.txt']\n",
        "\n",
        "for txt_path in txt_paths:\n",
        "    text_load = spark.read.csv(txt_path, header = False, inferSchema = True)\n",
        "    text_load = text_load.dropDuplicates()\n",
        "    file_name = txt_path.split('/')[-1][:-4] + '.parquet'\n",
        "    text_load.write.parquet(f's3a://kagglespotify6k/raw/{file_name}')\n",
        "    print(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bbd03f73-7d09-4284-a1ea-4840329bb2c8",
          "showTitle": false,
          "title": ""
        },
        "id": "x_7AbgarE1Gt"
      },
      "outputs": [],
      "source": [
        "# After some EDA, I noticed that the genres txt file is missing a significant amount of genres that are present in the main_dataset.csv\n",
        "# Will use the main_dataset to add the missing genres\n",
        "fix_genre_df =  spark.read.parquet('s3a://kagglespotify6k/raw/music_genres.parquet/')\n",
        "fix_genre_main = spark.read.parquet('s3a://kagglespotify6k/raw/main_dataset.parquet/')\n",
        "\n",
        "# Need to explode main_dataset's artists_genres column to get all the individual genres\n",
        "exploded_df = fix_genre_main.select(explode('artists_genres').alias('genre'))\n",
        "genres_from_main_df = exploded_df.distinct()\n",
        "\n",
        "# Get the genres from the raw parquet and rename the column\n",
        "pre_fix_genres_df = fix_genre_df.select('_c0').distinct()\n",
        "pre_fix_genres_df = pre_fix_genres_df.withColumnRenamed('_c0','genre')\n",
        "\n",
        "# Combine 2 dataframes to get new genres dataframe\n",
        "new_genres_df = pre_fix_genres_df.union(genres_from_main_df)\n",
        "\n",
        "# Write new_genres_df as a parquet to raw\n",
        "new_genres_df.write.mode('overwrite').parquet('s3a://kagglespotify6k/raw/fixed_genres_v1.parquet/')"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Milestone 4",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}